---
title: "Project 2: stat302project2 Tutorial"
author: "Lisa Jiang"
date: "12/11/2020"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{stat302project2 Tutorial}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```


# Part 1. R Package Development (45 points)

## Instructions

For this portion of Project 2, you are being asked to develop a well-documented, well-tested, and well-explained R package.
Follow the instruction on Lecture Slides 9 to set up the skeleton of your package.
This R package should be adjusted to include functions we've written throughout the class:

* `my_t.test`
* `my_lm`
* `my_knn_cv`
* `my_rf_cv`

Your package should include a detailed and thorough vignette, demonstrating use of all of these functions using the `gapminder` data from the `gapminder` package.

However, you must add and document the `gapminder` data to your own package and export it as the object `my_gapminder` (with proper credit in the documentation!).


Specifically, the vignette should have 5 parts:

1. A brief introductory paragraph explaining the package
  * This paragraph should include instructions for how to install your package from GitHub. 
  * The install instructions should be demonstrated with code that is not evaluated (`eval = FALSE`).
  * Make sure to include a call to `library()` for your package.

```{r install, eval = FALSE, results='hide'}
devtools::install_github("lisaxjiang/stat302project2")
```

Install packages and data:
```{r setup, results='hide'}
require(stat302project2)
library(stats)
library(dplyr, warn.conflicts = FALSE)
library(magrittr)
library(randomForest)
library(class)
library(ggplot2)
library(kableExtra)
library(tibble)
data("my_gapminder")
my_gapminder
data("my_penguins")
my_penguins <- na.omit(my_penguins)
```

2. A tutorial for `my_t.test`
  * Use the `lifeExp` data from `my_gapminder`.
  * Demonstrate a test of the hypothesis
  \begin{align}
  H_0: \mu &= 60,\\
  H_a: \mu &\neq 60.
  \end{align}
  * Demonstrate a test of the hypothesis
  \begin{align}
  H_0: \mu &= 60,\\
  H_a: \mu &< 60.
  \end{align}
  *  Demonstrate a test of the hypothesis
  \begin{align}
  H_0: \mu &= 60,\\
  H_a: \mu &> 60.
  \end{align}
  * For each of the tests above, carefully interpret the results using a p-value cut-off of $\alpha = 0.05$.
```{r}
two_sided_p <- my_t.test(my_gapminder[[4]], "two.sided", 60)$p_value
left_sided_p <- my_t.test(my_gapminder[[4]], "less", 60)$p_value
right_sided_p <- my_t.test(my_gapminder[[4]], "greater", 60)$p_value
two_sided_p
left_sided_p
right_sided_p
```

Test 1:

Since the p-value is over $\alpha=0.05$, we failed to reject the null hypothesis. Therefore, there isn't sufficient evidence that the population mean is not 60 years.

Test 2:

Since the p-value is under $\alpha=0.05$, we succeeded in rejecting the null hypothesis. Therefore, there is sufficient evidence that the populaion mean is less than 60 years.

Test 3:

Since the p-value is over $\alpha=0.05$, we failed to reject the null hypothesis. Therefore, there isn't sufficient evidence that the population mean is greater than 60 years.

  
3. A tutorial for `my_lm`
  * Demonstrate a regression using `lifeExp` as your response variable and `gdpPercap` and `continent` as explanatory variables.
  * Carefully interpret the `gdpPercap` coefficient.
  * Write the hypothesis test associated with the `gdpPercap` coefficient.
  * Carefully interpret the results the `gdpPercap` hypothesis test using a p-value cut-off of $\alpha = 0.05$.
  * Use `ggplot2` to plot the Actual vs. Fitted values.
  * Interpret the Actual vs. Fitted plot and make a statement on what it tells you about model fit.
```{r}
# extract life expectancy column from my_gapminder 
lifeExp <- my_gapminder[[4]]
# extract gdp_Percap column from my_gapminder 
gdp_Percap <- my_gapminder[[6]]
# extract the continent column from my_gapminder 
continent <- my_gapminder[[2]]

# demonstrate a regression using `lifeExp` as the response variable
lm <- my_lm(lifeExp ~ gdp_Percap + continent, data = my_gapminder)
summary(lm)
```

Hypothesis test:
  \begin{align}
  H_0: \beta &= 0,\\
  H_a: \beta &\neq 0.
  \end{align}

Interpretation of the hypothesis test:

The p-value for all factors is $0.001889$, which is smaller than $\alpha=0.05$. So there is sufficient evidence to reject the null hypothesis and conclude that the coefficient of this linear regression model is not zero.

```{r}
# get the intercept of the model
my_coef <- lm[, 1]
# create matrix and calculate fitted values
my_matrix <- model.matrix(lifeExp ~ gdp_Percap + continent, 
                          data = my_gapminder)
y_hat <- my_matrix %*% as.matrix(my_coef)
# create data frame for the dot plot
my_data <- data.frame("Actual" = my_gapminder$lifeExp,
                      "Fitted" = y_hat,
                      "Continent" = my_gapminder$continent)
# create the actual vs. fitted values plot
my_plot <- ggplot(my_data, aes(x = Actual, y = Fitted, color = Continent)) + 
  geom_point() + 
  geom_abline(slope = 1, intercept = 0, col = "red") + 
  labs(title = "Life Expectancy Actual v.s. Fitted Plot") + 
  theme_bw(base_size = 12) + 
  theme(plot.title = element_text(hjust = 0.5, face = "bold", size = 12))
my_plot
```

Actual vs. Fitted plot interpretation:

We can notice that Europe and Oceania has the highest overall life expectancy. Americas and Asia are just below them, while Africa tends to have the lowest life expectancy. Furthurmore, the red line on the plot is at $y=x$, and we can clearly see that the blue and purple dots follow the line closer. This suggests that if we are using life expectancy as our response variable with GDP per capita and continent as our explanatary variables, the prediction model is more useful in predicting life expectancy in Europe and Oceania.
  
4. A tutorial for `my_knn_cv` using `my_penguins`.
  * Predict output class `species` using covariates `bill_length_mm`, `bill_depth_mm`, `flipper_length_mm`, and `body_mass_g`. 
  * Use $5$-fold cross validation (`k_cv = 5`).
  * Iterate from `k_nn`$= 1,\ldots, 10$:
    * For each value of `k_nn`, record the training misclassification rate and the CV misclassification rate (output from your function).
  * State which model you would choose based on the training misclassification rates and which model you would choose based on the CV misclassification rates.
  * Discuss which model you would choose in practice and why. 
  * Your explanation(s) should include a general description of what the process of cross-validation is doing and why it is useful.
```{r}
# create an empty matrix to record indexes and outputs
my_matrix <- matrix(NA, nrow = 10, ncol = 3)
# run through the k_nn values
for (i in 1 : 10) {
  # the first row as the value of k_nn
  my_matrix[i, 1] <- i
  # get output list from my function
  my_output <- my_knn_cv(train = my_penguins[,3:6], 
                         cl = my_penguins$species, 
                         k_cv = 5, 
                         k_nn = i)
  # record predictions
  prediction <- my_output$class
  # get the training misclassification rate
  my_matrix[i, 2] <- sum(prediction != my_penguins$species) / length(prediction)
  # record cv misclassification rate
  my_matrix[i, 3] <- my_output$cv_error
}
# create data frame to hold columns from my matrix
my_data <- data.frame("k_nn" = my_matrix[, 1],
                      "training_misclassification_rate" = my_matrix[, 2],
                      "cv_misclassification_rate" = my_matrix[, 3])
my_data
```

Algorithm of cross validation:

In a k-fold cross validation, the data is first shuffled and split randomly into k parts. In each iteration, one of the k parts is choosen to be the testing data, while all other parts are used as training data. The training data is used to model a relationship between the variables data frame and the true values. Then, a prediction of a set of test values is made using the variables data frame in the choosen part. The predictions are compared with the true values of the test set and an evaluation score is recorded.

Why it is useful:

This method is useful because all parts among the k parts is used once as the prediction set. We can use this method to find a good balance between bias and variance.

Table of output interpretation:

Based on the training misclassification rate and cv misclassification rates in the table shown above, I would choose the 4th iteration where $k_nn=4$ in modeling the data of penguin because it has comparatively low rates in both the training misclassification rate and the lowest cv misclassification rate. The sum of the two is lowest at $k_nn=4$, therefore I think it has a decent tradeoff between bias and variance.

5. A tutorial for `my_rf_cv`
  * Predict `body_mass_g` using covariates `bill_length_mm`, `bill_depth_mm`, and `flipper_length_mm`. 
  * Iterate through `k` in  `c(2, 5, 10)`:
    * For each value of `k`, run your function $30$ times. 
    * For each of the $30$ iterations,. store the CV estimated MSE.
  * Use `ggplot2` with 3 boxplots to display these data in an informative way. There should be a boxplot associated with each value of `k`, representing $30$ simulations each.
  * Use a table to display the average CV estimate and the standard deviation of the CV estimates across $k$. Your table should have 3 rows (one for each value of $k$) and 2 columns (one for the mean and the other for the standard deviation).
  * Discuss the results you observe in the boxplots and table. Compare the means and standard deviations of the the different values of $k$ and comment on why you think this is the case.
  
```{r}
# create an empty matrix to hold MSE values
my_matrix <- matrix(NA, 90, 1)
# use for loop to run 30 iterations
for(i in 1 : 30) {
  # run function using k=2
  my_matrix[i, 1] <- my_rf_cv(2)
  # run function using k=5
  my_matrix[30+i, 1] <- my_rf_cv(5)
  # run function using k=2
  my_matrix[60+i, 1] <- my_rf_cv(10)
}

# calculate the mean and sd of the 30 simulations in k=2
mse2_mean <- mean(my_matrix[1 : 30, 1])
mse2_sd <- sd(my_matrix[1 : 30, 1])
# calculate the mean and sd of the 30 simulations in k=5
mse5_mean <- mean(my_matrix[31 : 60, 1])
mse5_sd <- sd(my_matrix[31 : 60, 1])
# calculate the mean and sd of the 30 simulations in k=10
mse10_mean <- mean(my_matrix[61 : 90, 1])
mse10_sd <- sd(my_matrix[61 : 90, 1])

# build a k input matrix as the index for my_matrix
k_matrix <- matrix(NA, 90, 1)
for(i in 1:30) {
  k_matrix[i, 1] <- "k = 2"
  k_matrix[30 + i, ] <- "k = 5"
  k_matrix[60 + i, ] <- "k = 10"
}

# create data frame to hold corresponding k value with cv_err
my_data <- data.frame(k_value = k_matrix[, 1], 
                      cv_err = my_matrix[, 1])

# create 3 boxplots with a boxplot associated with each value of k
my_boxplot <- ggplot(my_data, 
  aes(x = reorder(k_value, cv_err), y = cv_err)) + 
  geom_boxplot(fill = "light yellow") + 
  labs(title = "Cross validation error in k-alue equals 2, 5, and 10") +
  theme_bw(base_size = 12) +
  theme(plot.title = element_text(hjust = 0.5,
                                  face = "bold",
                                  size = 12)) + 
  xlab("k fold value") +
  ylab("Cross Validation Error")

# display boxplot
my_boxplot
```

```{r}
# create a table to display the average CV estimate and the standard deviation 
# of the CV estimates across k
my_table <- matrix(c(mse2_mean, mse2_sd, mse5_mean, mse5_sd, mse10_mean, 
                     mse10_sd), ncol = 2, byrow = TRUE)
# add row name for my_table
rownames(my_table) <- c("k = 2", "k = 5", "k = 10")
# add column name for my_table
colnames(my_table) <- c("mean", "sd")
# convert the matrix to a table and display the table
my_table <- as.table(my_table)
my_table
```

Interpretation for boxplots and table:

As k value increases, the mean and standard deviation of cross validation error both decreases. 

### Details

* Your package must be hosted on GitHub as a public repository. You are free to make the repository private after the course concludes.
* Your submission for this portion of the project will be a single link to your GitHub repository.
* Your `README.md` file should include badges for travis-ci automated testing and codecov code coverage.
* Your `README.md` should include an `Installation` section installation instructions and a `Use` section demonstrating how to view the package vignettes. 
See [my package](https://github.com/bryandmartin/corncob) for an example.
* Your package should pass all checks implemented in `devtools::check()`.
* Your code coverage should be at or near 100%. 
* Each of the 4 main functions should include at least one of `inference`/ `prediction` as `@keywords`, depending on what the function is primarily used for.
* Each function must include `@examples`.
* This is a software vignette, so all of your code should be displayed within your writing. Do not include a code appendix.
* All code and documentation should follow the style guidelines outlined in class.



# Part 2. Data Analysis Project Pipeline (15 points)

## Instructions

For this portion of Project 2, you are being asked to set up a GitHub repository demonstrating your ability to set up a systematic data analysis project workflow.
For this part, we are pretending we don't have a package and using code and analyses you have already generated for Part 1.

Your analysis should be contained on a GitHub repository and include:

* A `.Rproj` file with the name of the project.
* A `Data` subfolder with the raw, unprocessed data.
  * Within `Data`, save the `my_gapminder` and `my_penguins` data as a raw `.csv`. 
* A `Code` subfolder with code to be loaded by your analysis files.
  * Include `my_rf_cv.R` from your package in Part 1. You can include it exactly as it appears in your package, including documentation. Good `roxygen2` style documentation is not limited to packages!
* An `Analysis` subfolder.
  * Include a `.Rmd` file. This file can, for the most part, be a copy of part 5 from your package vignette. **However**, this R Markdown document must
    * load data directly from the `Data` subfolder,
    * use `source()` to source code directly from from the `Code` subfolder (your `.Rmd` should **not** include code generating the function `my_rf_cv`, it should load that function from your script!),
    * use `ggsave()` to save all your figures within your analysis scripts (remember, your relative path from files in `Analysis` will look like `"../Output/Figures"`).
    * use `saveRDS()` and `write_csv()` to save your table of summary statistics and your simulation results, respectively (see `Results` description).
* A `Output` subfolder with:
  * A `Figures` sub-subfolder with all the figures you generated in `Analysis`
  * A `Results` sub-subfolder that contains (a) your table of 8 summary statistics saved as a `.rds` file and (b) a `.csv` with your 120 simulated results with  4 columns for each value of $k$ and 30 rows for each simulation.
* A `.gitignore` file
  * Include `.Rproj.user` and `.Rhistory`

### Details

* Your data analysis project must be hosted on GitHub as a public repository. You are free to make the repository private after the course concludes.
* Your submission for this portion of the project will be a single link to your GitHub repository.
* Test whether your pipeline works. When you knit the `.Rmd` in your `Analysis` folder, it should re-load the `Data` and `Code` files and re-generate all the results in `Output`.
If your results in `Output` aren't systematically re-generated when you run your Analysis, something in your pipeline is broken!

